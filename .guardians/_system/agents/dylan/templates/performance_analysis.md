# ‚ö° Backend Performance Analysis & Optimization

**Reviewer:** Dylan (Backend Genius & Multi-Stack Architect)  
**Date:** 2025-11-14  
**Manager:** @tylerthibault  
**Focus:** Response Time, Memory Usage, Caching, Load Testing & Scaling

---

## Executive Summary
[Performance assessment covering response times, resource utilization, and scalability bottlenecks]

**Performance Health Score:** X/10
- **Response Times:** X/10 (API latency, database query speed, cache hit rates)
- **Resource Utilization:** X/10 (CPU, memory, I/O efficiency, connection pools)
- **Scalability:** X/10 (Load handling, horizontal scaling readiness, bottleneck identification)
- **Caching Strategy:** X/10 (Cache coverage, invalidation, hit rates, distributed caching)
- **Monitoring Coverage:** X/10 (APM integration, alerting, performance tracking)

**Key Findings:** X Critical | X High | X Medium | X Low

---

## Performance Environment Assessment

### Technology Stack Performance Profile
**Detected Technologies:**
- [ ] **Python**: Django, FastAPI, Flask (WSGI/ASGI performance characteristics)
- [ ] **Node.js**: Express, NestJS, Fastify (Event loop optimization)
- [ ] **C#**: ASP.NET Core, Kestrel (Thread pool management)
- [ ] **Load Balancer**: Nginx, HAProxy, AWS ALB/NLB
- [ ] **Caching**: Redis, Memcached, Application-level caching

**Performance Monitoring Stack:**
- [ ] **APM**: New Relic, DataDog, Application Insights
- [ ] **Metrics**: Prometheus + Grafana
- [ ] **Profiling**: py-spy, clinic.js, dotTrace

---

## Dylan's Performance Crime Scene Investigation

### üî¥ **Critical Performance Crimes**

#### 1. **Synchronous I/O Blocking**
**Crime:** Blocking the event loop or thread pool with synchronous operations
```python
# ‚ùå GUILTY - Blocking synchronous database calls
@app.route('/users')
def get_users():
    users = []
    for user_id in user_ids:
        # CRIME: Each DB call blocks execution
        user = db.session.query(User).get(user_id)  # 50ms per user
        user_data = fetch_user_profile(user.id)     # 100ms external API
        users.append({**user.to_dict(), **user_data})
    return jsonify(users)  # 150ms √ó 100 users = 15 SECONDS!
```

**Dylan's Fix:**
```python
# ‚úÖ REDEEMED - Asynchronous batch processing
import asyncio
import aiohttp
from sqlalchemy.ext.asyncio import AsyncSession

@app.route('/users')
async def get_users():
    async with AsyncSession(async_engine) as session:
        # Batch database query
        users = await session.execute(
            select(User).where(User.id.in_(user_ids))
        )
        user_list = users.scalars().all()
        
        # Parallel external API calls
        async with aiohttp.ClientSession() as http_session:
            tasks = [
                fetch_user_profile_async(http_session, user.id) 
                for user in user_list
            ]
            profiles = await asyncio.gather(*tasks)
        
        # Combine results
        result = [
            {**user.to_dict(), **profile} 
            for user, profile in zip(user_list, profiles)
        ]
        
        return jsonify(result)  # 200ms total (96% improvement!)

async def fetch_user_profile_async(session, user_id):
    async with session.get(f'/api/profiles/{user_id}') as response:
        return await response.json()
```

#### 2. **Memory Leaks and Inefficient Resource Management**
**Crime:** Loading massive datasets into memory without pagination
```python
# ‚ùå GUILTY - Memory massacre
@app.route('/export/users')
def export_users():
    # Loading 10M+ records into memory = OOM death
    all_users = User.query.all()  # 2GB+ of user objects
    
    csv_data = []
    for user in all_users:
        # More memory allocation per user
        user_orders = Order.query.filter_by(user_id=user.id).all()
        csv_data.append({
            'user': user.to_dict(),
            'orders': [order.to_dict() for order in user_orders]
        })
    
    return generate_csv(csv_data)  # System runs out of memory üí•
```

**Dylan's Fix:**
```python
# ‚úÖ REDEEMED - Streaming and memory-efficient processing
from flask import Response
import csv
from io import StringIO

@app.route('/export/users')
def export_users_streaming():
    def generate_csv_stream():
        # Stream header
        output = StringIO()
        writer = csv.writer(output)
        writer.writerow(['user_id', 'email', 'order_count', 'total_spent'])
        yield output.getvalue()
        output.truncate(0)
        output.seek(0)
        
        # Process in batches to control memory usage
        batch_size = 1000
        offset = 0
        
        while True:
            # Raw SQL with pagination for memory efficiency
            batch = db.session.execute(text("""
                SELECT 
                    u.id, u.email,
                    COUNT(o.id) as order_count,
                    COALESCE(SUM(o.total_amount), 0) as total_spent
                FROM users u
                LEFT JOIN orders o ON u.id = o.user_id
                GROUP BY u.id, u.email
                ORDER BY u.id
                LIMIT :limit OFFSET :offset
            """), {'limit': batch_size, 'offset': offset})
            
            rows = batch.fetchall()
            if not rows:
                break
            
            # Write batch to CSV
            for row in rows:
                writer.writerow([row.id, row.email, row.order_count, row.total_spent])
            
            yield output.getvalue()
            output.truncate(0)
            output.seek(0)
            
            offset += batch_size
            
            # Force garbage collection for long-running streams
            if offset % 10000 == 0:
                import gc
                gc.collect()
    
    return Response(
        generate_csv_stream(),
        mimetype='text/csv',
        headers={'Content-Disposition': 'attachment; filename=users.csv'}
    )
```

#### 3. **Cache Stampede and Cache Misses**
**Crime:** No caching strategy or cache stampede scenarios
```python
# ‚ùå GUILTY - Cache catastrophe
@app.route('/popular-products')
def get_popular_products():
    # Expensive query runs every single request
    products = db.session.execute(text("""
        SELECT p.*, COUNT(oi.id) as purchase_count
        FROM products p
        JOIN order_items oi ON p.id = oi.product_id
        JOIN orders o ON oi.order_id = o.id
        WHERE o.created_at >= NOW() - INTERVAL 30 DAY
        GROUP BY p.id
        ORDER BY purchase_count DESC
        LIMIT 20
    """)).fetchall()  # 2.5 second query √ó 1000 requests = server meltdown
    
    return jsonify([dict(row) for row in products])
```

**Dylan's Fix:**
```python
# ‚úÖ REDEEMED - Multi-layer caching strategy
import redis
import pickle
from functools import wraps
import hashlib

redis_client = redis.Redis(host='localhost', port=6379, db=0)

def cache_result(ttl=300, prefix=''):
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            # Generate cache key from function name and arguments
            key_data = f"{prefix}:{func.__name__}:{args}:{sorted(kwargs.items())}"
            cache_key = hashlib.md5(key_data.encode()).hexdigest()
            
            # Try cache first
            cached_result = redis_client.get(cache_key)
            if cached_result:
                return pickle.loads(cached_result)
            
            # Cache miss - execute function
            result = func(*args, **kwargs)
            
            # Store in cache with TTL
            redis_client.setex(
                cache_key, 
                ttl, 
                pickle.dumps(result)
            )
            
            return result
        return wrapper
    return decorator

@app.route('/popular-products')
@cache_result(ttl=900, prefix='products')  # 15 minute cache
def get_popular_products():
    # This expensive query now runs max once every 15 minutes
    products = db.session.execute(text("""
        SELECT p.*, COUNT(oi.id) as purchase_count
        FROM products p
        JOIN order_items oi ON p.id = oi.product_id
        JOIN orders o ON oi.order_id = o.id
        WHERE o.created_at >= NOW() - INTERVAL 30 DAY
        GROUP BY p.id
        ORDER BY purchase_count DESC
        LIMIT 20
    """)).fetchall()
    
    return jsonify([dict(row) for row in products])

# Cache warming strategy
@app.cli.command()
def warm_cache():
    """Warm up critical caches during off-peak hours"""
    logger.info("Starting cache warming process")
    
    # Pre-populate expensive queries
    get_popular_products()
    get_user_recommendations()
    get_trending_categories()
    
    logger.info("Cache warming completed")
```

### üü† **High Priority Issues**

#### 4. **Inefficient Serialization**
**Crime:** Over-serializing or using inefficient serialization
```python
# ‚ùå GUILTY - Serialization slowdown
@app.route('/users')
def get_users():
    users = User.query.all()
    # Converting to dict is expensive for large objects
    return jsonify([user.to_dict() for user in users])  # 500ms for 1000 users

class User(db.Model):
    def to_dict(self):
        # Expensive: Creates dict for every field, including relationships
        return {c.name: getattr(self, c.name) for c in self.__table__.columns}
```

**Dylan's Fix:**
```python
# ‚úÖ REDEEMED - Optimized serialization with marshmallow
from marshmallow import Schema, fields
from flask import jsonify

class UserSchema(Schema):
    class Meta:
        # Only serialize needed fields
        fields = ('id', 'username', 'email', 'created_at')
    
    # Format dates efficiently
    created_at = fields.DateTime(format='%Y-%m-%d')

user_schema = UserSchema()
users_schema = UserSchema(many=True)

@app.route('/users')
def get_users():
    # Use database-level projection for efficiency
    users = db.session.query(
        User.id, User.username, User.email, User.created_at
    ).all()
    
    # Fast serialization with marshmallow
    result = users_schema.dump(users)  # 50ms for 1000 users (90% improvement)
    
    return jsonify(result)

# Alternative: Custom fast serializer for maximum performance
class FastUserSerializer:
    @staticmethod
    def serialize_users(users):
        return [
            {
                'id': user.id,
                'username': user.username,
                'email': user.email,
                'created_at': user.created_at.isoformat()
            }
            for user in users
        ]  # 20ms for 1000 users (96% improvement)
```

#### 5. **Database Connection Pool Exhaustion**
**Crime:** Not properly managing database connections under load
```python
# ‚ùå GUILTY - Connection chaos
# Default SQLAlchemy setup
engine = create_engine('postgresql://...')  # Default pool_size=5, max_overflow=10

@app.route('/heavy-operation')
def heavy_operation():
    # Each request holds connection for entire operation
    users = User.query.all()
    for user in users:
        # Long-running operation holding DB connection
        process_user_data(user)  # 30 seconds per user
        update_user_stats(user)
    # Connection held for potentially hours!
```

**Dylan's Fix:**
```python
# ‚úÖ REDEEMED - Proper connection management
from sqlalchemy import create_engine
from sqlalchemy.pool import QueuePool
from contextlib import contextmanager

# Optimized connection pool configuration
engine = create_engine(
    'postgresql://...',
    poolclass=QueuePool,
    pool_size=20,              # Base connections
    max_overflow=30,           # Burst capacity
    pool_pre_ping=True,        # Validate connections
    pool_recycle=3600,         # Recycle every hour
    echo_pool=True             # Log pool events
)

@contextmanager
def get_db_session():
    """Context manager for automatic session cleanup"""
    session = Session(engine)
    try:
        yield session
        session.commit()
    except Exception:
        session.rollback()
        raise
    finally:
        session.close()

@app.route('/heavy-operation')
def heavy_operation_optimized():
    # Batch process with connection management
    batch_size = 100
    processed = 0
    
    while True:
        # Get batch with minimal connection time
        with get_db_session() as session:
            users = session.query(User).offset(processed).limit(batch_size).all()
            if not users:
                break
        
        # Process batch without holding database connection
        for user in users:
            process_user_data(user)  # No DB connection held
        
        # Update in batch
        with get_db_session() as session:
            for user in users:
                update_user_stats_batch(session, user)
        
        processed += batch_size
    
    return jsonify({'processed': processed})
```

---

## Multi-Stack Performance Optimization

### Python (FastAPI + asyncio)
```python
# High-performance async API with FastAPI
import asyncio
import aioredis
from fastapi import FastAPI, BackgroundTasks
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
import uvloop  # High-performance event loop

# Configure uvloop for better performance
asyncio.set_event_loop_policy(uvloop.EventLoopPolicy())

app = FastAPI()

# Async database engine
async_engine = create_async_engine(
    "postgresql+asyncpg://user:pass@localhost/db",
    pool_size=20,
    max_overflow=0,
    echo=False
)

# Redis connection pool
redis_pool = None

@app.on_event("startup")
async def startup():
    global redis_pool
    redis_pool = aioredis.ConnectionPool.from_url(
        "redis://localhost", 
        max_connections=10
    )

@app.middleware("http")
async def add_process_time_header(request, call_next):
    start_time = time.time()
    response = await call_next(request)
    process_time = time.time() - start_time
    response.headers["X-Process-Time"] = str(process_time)
    return response

@app.get("/users/{user_id}/analytics")
async def get_user_analytics(user_id: int):
    async with AsyncSession(async_engine) as session:
        # Concurrent database and cache operations
        user_task = asyncio.create_task(get_user_async(session, user_id))
        cache_task = asyncio.create_task(get_cached_analytics(user_id))
        
        user, cached_analytics = await asyncio.gather(user_task, cache_task)
        
        if cached_analytics:
            return cached_analytics
        
        # Compute analytics asynchronously
        analytics = await compute_user_analytics(session, user_id)
        
        # Cache result in background
        asyncio.create_task(cache_analytics(user_id, analytics))
        
        return analytics

async def get_user_async(session: AsyncSession, user_id: int):
    result = await session.execute(
        select(User).where(User.id == user_id)
    )
    return result.scalar_one_or_none()

async def compute_user_analytics(session: AsyncSession, user_id: int):
    # Parallel queries for different analytics
    tasks = [
        get_order_stats(session, user_id),
        get_engagement_metrics(session, user_id),
        get_recommendation_scores(session, user_id)
    ]
    
    order_stats, engagement, recommendations = await asyncio.gather(*tasks)
    
    return {
        "order_statistics": order_stats,
        "engagement_metrics": engagement,
        "recommendations": recommendations,
        "computed_at": datetime.utcnow().isoformat()
    }
```

### Node.js (Express + Clustering)
```javascript
// High-performance Node.js with clustering and optimization
const cluster = require('cluster');
const numCPUs = require('os').cpus().length;
const express = require('express');
const Redis = require('ioredis');
const { performance } = require('perf_hooks');

if (cluster.isMaster) {
    console.log(`Master ${process.pid} is running`);
    
    // Fork workers
    for (let i = 0; i < numCPUs; i++) {
        cluster.fork();
    }
    
    cluster.on('exit', (worker, code, signal) => {
        console.log(`Worker ${worker.process.pid} died`);
        cluster.fork(); // Restart worker
    });
} else {
    const app = express();
    
    // Redis cluster for caching
    const redis = new Redis.Cluster([
        { host: 'localhost', port: 7000 },
        { host: 'localhost', port: 7001 },
    ]);
    
    // Connection pooling for database
    const { Pool } = require('pg');
    const pool = new Pool({
        connectionString: process.env.DATABASE_URL,
        max: 20,
        idleTimeoutMillis: 30000,
        connectionTimeoutMillis: 2000,
    });
    
    // Performance monitoring middleware
    app.use((req, res, next) => {
        req.startTime = performance.now();
        res.on('finish', () => {
            const duration = performance.now() - req.startTime;
            console.log(`${req.method} ${req.path} - ${duration.toFixed(2)}ms`);
        });
        next();
    });
    
    // Optimized bulk operations
    app.get('/users/bulk-analytics', async (req, res) => {
        const userIds = req.query.user_ids.split(',').map(Number);
        
        try {
            // Batch database query
            const query = `
                SELECT 
                    u.id,
                    u.username,
                    COUNT(o.id) as order_count,
                    SUM(o.total_amount) as total_spent,
                    MAX(o.created_at) as last_order
                FROM users u
                LEFT JOIN orders o ON u.id = o.user_id
                WHERE u.id = ANY($1)
                GROUP BY u.id, u.username
            `;
            
            const result = await pool.query(query, [userIds]);
            
            // Parallel cache operations
            const cachePromises = result.rows.map(user => 
                redis.setex(`user:analytics:${user.id}`, 3600, JSON.stringify(user))
            );
            
            await Promise.all(cachePromises);
            
            res.json({
                users: result.rows,
                cached: true,
                processing_time: performance.now() - req.startTime
            });
            
        } catch (error) {
            console.error('Bulk analytics error:', error);
            res.status(500).json({ error: 'Internal server error' });
        }
    });
    
    // Stream processing for large datasets
    const { Transform } = require('stream');
    
    app.get('/export/orders/stream', (req, res) => {
        res.setHeader('Content-Type', 'application/json');
        res.setHeader('Transfer-Encoding', 'chunked');
        
        const query = new QueryStream(`
            SELECT o.*, u.username 
            FROM orders o 
            JOIN users u ON o.user_id = u.id 
            ORDER BY o.created_at DESC
        `);
        
        const client = await pool.connect();
        const stream = client.query(query);
        
        const jsonTransform = new Transform({
            objectMode: true,
            transform(chunk, encoding, callback) {
                callback(null, JSON.stringify(chunk) + '\n');
            }
        });
        
        stream
            .pipe(jsonTransform)
            .pipe(res)
            .on('end', () => client.release())
            .on('error', (err) => {
                console.error('Stream error:', err);
                client.release();
            });
    });
    
    const PORT = process.env.PORT || 3000;
    app.listen(PORT, () => {
        console.log(`Worker ${process.pid} started on port ${PORT}`);
    });
}
```

### C# (ASP.NET Core + Performance Optimization)
```csharp
// Program.cs - High-performance configuration
var builder = WebApplication.CreateBuilder(args);

// Kestrel performance tuning
builder.WebHost.ConfigureKestrel(options =>
{
    options.Limits.MaxConcurrentConnections = 1000;
    options.Limits.MaxConcurrentUpgradedConnections = 1000;
    options.Limits.MaxRequestBodySize = 52428800; // 50MB
    options.Limits.RequestHeadersTimeout = TimeSpan.FromSeconds(30);
});

// Database performance configuration
builder.Services.AddDbContext<ApplicationDbContext>(options =>
{
    options.UseSqlServer(connectionString, sqlOptions =>
    {
        sqlOptions.CommandTimeout(30);
        sqlOptions.EnableRetryOnFailure(maxRetryCount: 3);
    });
    options.EnableSensitiveDataLogging(false);
    options.EnableServiceProviderCaching();
});

// Redis caching
builder.Services.AddStackExchangeRedisCache(options =>
{
    options.Configuration = "localhost:6379";
    options.InstanceName = "MyApp";
});

// Memory caching
builder.Services.AddMemoryCache(options =>
{
    options.SizeLimit = 1024; // 1GB limit
});

// HTTP client with connection pooling
builder.Services.AddHttpClient("ApiClient", client =>
{
    client.Timeout = TimeSpan.FromSeconds(30);
}).ConfigurePrimaryHttpMessageHandler(() => new HttpClientHandler()
{
    MaxConnectionsPerServer = 10
});

// Services/UserAnalyticsService.cs - Optimized data processing
public class UserAnalyticsService
{
    private readonly ApplicationDbContext _context;
    private readonly IDistributedCache _cache;
    private readonly ILogger<UserAnalyticsService> _logger;
    
    public async Task<UserAnalyticsDto> GetUserAnalyticsAsync(int userId)
    {
        var cacheKey = $"user:analytics:{userId}";
        
        // Try cache first
        var cachedResult = await _cache.GetStringAsync(cacheKey);
        if (cachedResult != null)
        {
            return JsonSerializer.Deserialize<UserAnalyticsDto>(cachedResult);
        }
        
        // Parallel database queries
        var userTask = GetUserAsync(userId);
        var orderStatsTask = GetOrderStatisticsAsync(userId);
        var engagementTask = GetEngagementMetricsAsync(userId);
        
        await Task.WhenAll(userTask, orderStatsTask, engagementTask);
        
        var analytics = new UserAnalyticsDto
        {
            User = await userTask,
            OrderStatistics = await orderStatsTask,
            EngagementMetrics = await engagementTask,
            ComputedAt = DateTime.UtcNow
        };
        
        // Cache result
        var cacheOptions = new DistributedCacheEntryOptions
        {
            AbsoluteExpirationRelativeToNow = TimeSpan.FromMinutes(15)
        };
        
        await _cache.SetStringAsync(
            cacheKey, 
            JsonSerializer.Serialize(analytics), 
            cacheOptions
        );
        
        return analytics;
    }
    
    public async Task<List<UserBulkAnalyticsDto>> GetBulkAnalyticsAsync(List<int> userIds)
    {
        // Batch query optimization
        var query = _context.Users
            .Where(u => userIds.Contains(u.Id))
            .Select(u => new UserBulkAnalyticsDto
            {
                UserId = u.Id,
                Username = u.Username,
                OrderCount = u.Orders.Count(),
                TotalSpent = u.Orders.Sum(o => o.TotalAmount),
                LastOrderDate = u.Orders.Max(o => o.CreatedAt),
                AvgOrderValue = u.Orders.Average(o => o.TotalAmount)
            })
            .AsNoTracking(); // Read-only optimization
        
        return await query.ToListAsync();
    }
    
    // Streaming for large datasets
    public async IAsyncEnumerable<OrderExportDto> StreamOrdersAsync(
        DateTime startDate, 
        [EnumeratorCancellation] CancellationToken cancellationToken = default)
    {
        var batchSize = 1000;
        var skip = 0;
        
        while (!cancellationToken.IsCancellationRequested)
        {
            var batch = await _context.Orders
                .Where(o => o.CreatedAt >= startDate)
                .OrderBy(o => o.CreatedAt)
                .Skip(skip)
                .Take(batchSize)
                .Select(o => new OrderExportDto
                {
                    Id = o.Id,
                    UserId = o.UserId,
                    TotalAmount = o.TotalAmount,
                    Status = o.Status,
                    CreatedAt = o.CreatedAt
                })
                .AsNoTracking()
                .ToListAsync(cancellationToken);
            
            if (!batch.Any())
                yield break;
            
            foreach (var order in batch)
            {
                yield return order;
            }
            
            skip += batchSize;
            
            // Allow other operations to run
            await Task.Delay(1, cancellationToken);
        }
    }
}
```

---

## Load Testing & Benchmarking

### Performance Testing Strategy
```python
# Load testing with locust
from locust import HttpUser, task, between
import random

class UserBehavior(HttpUser):
    wait_time = between(1, 3)
    
    def on_start(self):
        # Login once per user
        response = self.client.post("/auth/login", json={
            "email": f"user{random.randint(1, 10000)}@test.com",
            "password": "testpass123"
        })
        self.token = response.json().get("token")
        self.headers = {"Authorization": f"Bearer {self.token}"}
    
    @task(3)
    def view_dashboard(self):
        """Most common operation"""
        self.client.get("/api/dashboard", headers=self.headers)
    
    @task(2)
    def list_orders(self):
        """Common operation"""
        self.client.get("/api/orders", headers=self.headers)
    
    @task(1)
    def create_order(self):
        """Less frequent but important operation"""
        order_data = {
            "items": [
                {"product_id": random.randint(1, 100), "quantity": random.randint(1, 5)}
            ],
            "shipping_address": "123 Test St"
        }
        self.client.post("/api/orders", json=order_data, headers=self.headers)
    
    @task(1)
    def search_products(self):
        """Search load testing"""
        search_terms = ["electronics", "clothing", "books", "sports"]
        term = random.choice(search_terms)
        self.client.get(f"/api/products/search?q={term}", headers=self.headers)

# Run with: locust -f load_test.py --host=http://localhost:8000
```

### Performance Monitoring Setup
```python
# APM integration with custom metrics
import time
import psutil
from prometheus_client import Counter, Histogram, Gauge, start_http_server

# Custom metrics
request_duration = Histogram('http_request_duration_seconds', 'HTTP request duration')
request_count = Counter('http_requests_total', 'Total HTTP requests', ['method', 'endpoint', 'status'])
active_connections = Gauge('active_database_connections', 'Active DB connections')
memory_usage = Gauge('memory_usage_bytes', 'Memory usage in bytes')
cpu_usage = Gauge('cpu_usage_percent', 'CPU usage percentage')

class PerformanceMonitor:
    def __init__(self):
        self.start_metrics_server()
    
    def start_metrics_server(self):
        # Start Prometheus metrics server
        start_http_server(8000)
    
    def monitor_system_metrics(self):
        """Collect system performance metrics"""
        while True:
            # Memory usage
            memory = psutil.virtual_memory()
            memory_usage.set(memory.used)
            
            # CPU usage
            cpu_percent = psutil.cpu_percent(interval=1)
            cpu_usage.set(cpu_percent)
            
            # Database connections
            db_connections = get_active_db_connections()
            active_connections.set(db_connections)
            
            time.sleep(30)  # Collect every 30 seconds
    
    @staticmethod
    def time_request(func):
        """Decorator to time request execution"""
        def wrapper(*args, **kwargs):
            start_time = time.time()
            try:
                result = func(*args, **kwargs)
                status = '200'
                return result
            except Exception as e:
                status = '500'
                raise
            finally:
                duration = time.time() - start_time
                request_duration.observe(duration)
                request_count.labels(
                    method=request.method,
                    endpoint=request.endpoint,
                    status=status
                ).inc()
        return wrapper
```

---

## Caching Strategies

### Multi-Level Caching Implementation
```python
# Advanced caching with multiple layers
import redis
import pickle
from functools import wraps
from typing import Optional, Any
import hashlib

class MultiLevelCache:
    def __init__(self):
        self.redis_client = redis.Redis(host='localhost', port=6379, decode_responses=False)
        self.local_cache = {}  # In-memory L1 cache
        self.local_cache_size = 1000  # Max items in local cache
    
    def get(self, key: str) -> Optional[Any]:
        # L1 Cache (local memory)
        if key in self.local_cache:
            return self.local_cache[key]['value']
        
        # L2 Cache (Redis)
        cached_data = self.redis_client.get(key)
        if cached_data:
            value = pickle.loads(cached_data)
            # Promote to L1 cache
            self._set_local(key, value)
            return value
        
        return None
    
    def set(self, key: str, value: Any, ttl: int = 300):
        # Set in both caches
        self._set_local(key, value)
        serialized_value = pickle.dumps(value)
        self.redis_client.setex(key, ttl, serialized_value)
    
    def _set_local(self, key: str, value: Any):
        # LRU eviction for local cache
        if len(self.local_cache) >= self.local_cache_size:
            # Remove oldest entry
            oldest_key = min(self.local_cache.keys(), 
                           key=lambda k: self.local_cache[k]['timestamp'])
            del self.local_cache[oldest_key]
        
        self.local_cache[key] = {
            'value': value,
            'timestamp': time.time()
        }

# Cache warming and invalidation
class CacheManager:
    def __init__(self, cache: MultiLevelCache):
        self.cache = cache
    
    def warm_user_cache(self, user_id: int):
        """Pre-populate cache with user data"""
        user_data = self._fetch_user_complete(user_id)
        
        # Cache individual components
        self.cache.set(f'user:{user_id}', user_data['user'], ttl=3600)
        self.cache.set(f'user:{user_id}:orders', user_data['orders'], ttl=1800)
        self.cache.set(f'user:{user_id}:preferences', user_data['preferences'], ttl=7200)
    
    def invalidate_user_cache(self, user_id: int):
        """Remove all cached data for a user"""
        keys_to_invalidate = [
            f'user:{user_id}',
            f'user:{user_id}:orders',
            f'user:{user_id}:preferences',
            f'user:{user_id}:analytics'
        ]
        
        for key in keys_to_invalidate:
            self.cache.redis_client.delete(key)
            if key in self.cache.local_cache:
                del self.cache.local_cache[key]
    
    def cache_aside_pattern(self, cache_key: str, fetch_func, ttl: int = 300):
        """Cache-aside pattern implementation"""
        # Try cache first
        result = self.cache.get(cache_key)
        if result is not None:
            return result
        
        # Cache miss - fetch from source
        result = fetch_func()
        
        # Store in cache
        if result is not None:
            self.cache.set(cache_key, result, ttl)
        
        return result
```

### Cache Invalidation Strategies
```python
# Event-driven cache invalidation
from typing import List
import json

class CacheInvalidator:
    def __init__(self, cache_manager: CacheManager):
        self.cache_manager = cache_manager
        self.invalidation_patterns = {
            'user_updated': ['user:{user_id}', 'user:{user_id}:*'],
            'order_created': ['user:{user_id}:orders', 'popular_products', 'order_stats'],
            'product_updated': ['product:{product_id}', 'popular_products', 'category:{category_id}'],
        }
    
    def handle_event(self, event_type: str, event_data: dict):
        """Handle domain events and invalidate appropriate caches"""
        patterns = self.invalidation_patterns.get(event_type, [])
        
        for pattern in patterns:
            # Replace placeholders with actual values
            cache_keys = self._resolve_pattern(pattern, event_data)
            
            for cache_key in cache_keys:
                self.cache_manager.cache.redis_client.delete(cache_key)
    
    def _resolve_pattern(self, pattern: str, data: dict) -> List[str]:
        """Resolve cache key patterns with actual data"""
        if '{' not in pattern:
            return [pattern]
        
        # Handle wildcard patterns
        if pattern.endswith('*'):
            prefix = pattern[:-1].format(**data)
            keys = self.cache_manager.cache.redis_client.keys(f"{prefix}*")
            return [key.decode() for key in keys]
        
        # Handle specific patterns
        return [pattern.format(**data)]

# Usage in application events
@app.after_request
def handle_cache_invalidation(response):
    if hasattr(g, 'cache_events'):
        for event in g.cache_events:
            cache_invalidator.handle_event(event['type'], event['data'])
    return response

# In your business logic
def update_user_profile(user_id: int, profile_data: dict):
    # Update database
    user = update_user_in_db(user_id, profile_data)
    
    # Queue cache invalidation
    if not hasattr(g, 'cache_events'):
        g.cache_events = []
    
    g.cache_events.append({
        'type': 'user_updated',
        'data': {'user_id': user_id}
    })
    
    return user
```

---

## Dylan's Performance Optimization Checklist

### Response Time Optimization
- [ ] **Database queries optimized** - Proper indexing, query analysis
- [ ] **N+1 queries eliminated** - Eager loading, batch operations
- [ ] **Caching implemented** - Multi-level, appropriate TTL
- [ ] **Async operations used** - Non-blocking I/O where possible
- [ ] **Connection pooling configured** - Optimal pool sizes

### Memory Management
- [ ] **Memory leaks identified** - Profiling, monitoring
- [ ] **Large dataset streaming** - Pagination, chunked processing
- [ ] **Efficient serialization** - Minimal object creation
- [ ] **Garbage collection tuned** - Language-specific optimization
- [ ] **Resource cleanup implemented** - Context managers, using statements

### Scalability Preparation
- [ ] **Load testing completed** - Realistic traffic simulation
- [ ] **Bottlenecks identified** - Performance profiling
- [ ] **Horizontal scaling ready** - Stateless design, load balancing
- [ ] **Database scaling planned** - Read replicas, sharding strategy
- [ ] **Monitoring comprehensive** - APM, metrics, alerting

### Caching Strategy
- [ ] **Cache hit rates monitored** - Measure effectiveness
- [ ] **Cache invalidation strategy** - Event-driven, appropriate TTL
- [ ] **Cache warming implemented** - Pre-populate critical data
- [ ] **Cache stampede prevention** - Locking, staggered updates
- [ ] **Multi-level caching** - Memory + distributed caching

---

## Dylan's Performance Wisdom

*"Performance is not just about making things faster‚Äîit's about making them sustainably fast. A system that screams under normal load but crawls under peak traffic is worse than one that's consistently good. Measure everything, optimize the bottlenecks, and always remember: premature optimization is the root of all evil, but so is premature pessimization. Profile first, then optimize with purpose."*

**The Performance Golden Rules:**
1. **Measure before optimizing** - Data beats assumptions every time
2. **Optimize the bottleneck** - Fix the slowest link first
3. **Cache strategically** - Right data, right level, right TTL
4. **Scale horizontally** - Stateless services, shared-nothing architecture
5. **Monitor religiously** - Performance degrades silently over time
6. **Load test realistically** - Real traffic patterns, real data volumes

---

*Remember: Performance is a feature, not an afterthought. Users will forgive ugly interfaces before they forgive slow ones. Make your backend fast, reliable, and scalable‚Äîthen make it faster.*